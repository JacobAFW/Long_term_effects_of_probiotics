---
title: "Pipeline"
output: pdf_document
author: "Jacob Westaway"
date: "Last updated on `r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/Users/Jacob/Desktop/Jacob_Uni/Data/Long_term_effects_of_probiotics')
```


# About.

This document contains a pipeline to go from raw Illumina MiSeq reads to a phyloseq object (along with some additional exploratory analysis) and is based on the workflow from the paper [The bacterial gut microbiome of probiotic-treated very-preterm infants â€“ Changes from admission to discharge](https://github.com/JacobAFW/NICU_Microbiome_Study/), which was based largely around this [DADA2](https://pubmed.ncbi.nlm.nih.gov/27508062/) workflow developed by *Callahan, et al.*.


## Load required packages.

```{r, warning=F, message=F, results='hide'}
sapply(c("dada2", "phyloseq", "DECIPHER", "phangorn", "BiocManager", "BiocStyle", 
        "Biostrings", "ShortRead", "ggplot2", "gridExtra", "tibble", "tidyverse"), 
        require, character.only = TRUE)
```

## Read quality.

### Organise forward and reverse fastq filenames into own lists (check file format).
 - First define the file path to the directory containing the fastq files (we will use this several times).
 
```{r, warning=F, message=F, eval=F}
path <-"data/fastq"

fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))

fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))
```

### Extract sample names.

```{r, warning=F, message=F, eval=F}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

\newpage

### Check quality of Forward and Reverse Reads (used to define truncLen in filtering).

```{r, warning=F, fig.cap="Quality of forward reads.", message=F}
plotQualityProfile(fnFs[1:2])
```

```{r, warning=F, fig.cap="Quality of reverse reads.", message=F}
plotQualityProfile(fnRs[1:2])
```

### Assign names for filtered reads.

```{r, warning=F, message=F,eval=F}
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))

filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
```

### Filter and trim the reads. 
 - Paremeters based on data and quality plots. 
 - `truncLean` defined by when quality plots begin to drop off, but ensuring it is large enough to maintain read overlap (=>20bp) downstream.
 - `trimLeft` is not needed if primers/barcodes already removed.
 - `maxEE = c(2,2)` is for filtering, where the higher the value the more relaxed filtering,allowing more reads to get through. 
 - Good quality data should allow for more stringent parameters (2 is stringent).
 - The number of reads filtered is checked. If reads are too low, can alter parameters.
 
```{r, warning=F, message=F, eval=F}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = c(275,250), 
                     trimLeft = c(16,21), 
                     maxN = 0, 
                     maxEE = c(2,2), 
                     truncQ = 2, 
                     rm.phix = TRUE,
                     compress = TRUE, 
                     multithread = FALSE)# windows can't support multithread
head(out)
out
```

## Infer sequence variants.

### Calculate Error Rates.
 - Error rates are used for sample ineference downstream.
 
```{r, warning=F, message=F, results='hide', eval=F}
errF <- learnErrors(filtFs, multithread = TRUE)

errR <- learnErrors(filtRs, multithread = TRUE)
```

### Plot error rates. 
 - Estimated error rates (black line) should be a good fit to observed rates (points) and error should decrease.
 
```{r, warning=F, fig.cap="Error rates for forward reads", message=F}
plotErrors(errF, nominalQ = TRUE)
```

```{r, warning=F, fig.cap="Error rates for reverse reads.", message=F}
plotErrors(errR, nominalQ = TRUE)
```

### Dereplication.
 - Combine indentical sequences into unique sequence bins.
 - Name the derep-class objects by the sample name.
 
```{r, warning=F, message=F,eval=F}
derepFs <- derepFastq(filtFs, verbose = TRUE)

derepRs <- derepFastq(filtRs, verbose = TRUE)

names(derepFs) <- sample.names

names(derepRs) <- sample.names
```

### Sequence Inference.

```{r, warning=F, message=F, results='hide',eval=F}
dadaFs <- dada(derepFs, err = errF, multithread = F)

dadaRs <- dada(derepRs, err = errR, multithread = F)
```

### Inspect denoised data.

```{r, warning=F, message=F, results='hide',eval=F}
dadaFs[[1]]

dadaRs[[1]]
```

### Merge Paired Reads and inspect merged data.
 - Removes paired reads that do not perfectly overlap.
 - Arguments represent infered samples AND denoised reads.
 
```{r, warning=F, message=F, results='hide',eval=F}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)
```

## Construct amplicon sequence variance (ASV) table and remove chimeras.

### Construct ASV table.
 - Check dimentions and inspect distribution of sequence lengths.
 
```{r, warning=F, message=F, results='hide',eval=F}
seqtab <- makeSequenceTable(mergers)

dim(seqtab)
```

### Remove chimeras.

```{r, warning=F, message=F,eval=F}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", 
                                    multithread = TRUE, verbose = TRUE)
```

### Track reads through pipeline.

```{r, warning=F, message=F}
getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), 
               sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

### Comapring different truncation parameters
```{r}
#trunc_270F_250R <- seqtab.nochim
#trunc_250F_220R <- seqtab.nochim
trunc_260F_220R <- read_csv("data/seqtab/seqtab_nochim_260F_220R.csv") %>% # this was run on a HPC server
  cbind(
    trunc_250F_250R %>% 
      as.data.frame() %>% 
      rownames_to_column("ID") %>% 
      select(ID)
  ) %>% 
  column_to_rownames("ID") %>% 
  as.matrix()
```

**I went with the 260F 220R dataset, as it allowed sufficient overlap whilst still being strict on poor quality R reads.**

## Contamination removal with [MicroDecon](https://onlinelibrary.wiley.com/doi/full/10.1002/edn3.11).

```{r, warning=F, message=F}
library(microDecon)
```
 
### Read in metadata (needed for MicroDecon)

```{r,warning=F,message=F,eval=F}
Metadata  <- readxl::read_excel("data/metadata/Clinical_Metadata_Template_2020.xlsx") %>% 
  select(ID, Extraction, OMNIgene_Number) %>% 
  filter(ID != "NO SAMPLE")
```


### Reformat data for *MicroDecon*.
 - Function is **data specific**.
 - Transpose sequencing table (post chimera removal) and convert to a dataframe.
 - Reorder sequencing table by a prior grouping (days).
 - Move blank sample columns to the start of the sequencing table.
 - Turn row names into their own column as *MicroDecon* requires that the OTUs have a unique ID in column 1.
 
```{r,warning=F,message=F,eval=F}
microdecon.df <- t(trunc_260F_220R) %>%
  as.data.frame() %>% 
  relocate("320", "340", "350") %>% 
  rownames_to_column(var = "ID") 
```


### Decontaminate data using `decon()`.
 - `numb.ind` is the number of columns for each priori grouping.
 - `taxa = F` as there is no taxonomy in the dataframe.
 
```{r, warning=F, message=F,eval=F}
decontaminated <- decon(data = microdecon.df, numb.blanks = 3, 
                  numb.ind = c(19, 19, 9, 2), taxa = F)
```

#### Check *MicroDecon* Outputs.

```{r, eval=F, message=F}
decontaminated$decon.table
decontaminated$reads.removed
decontaminated$OTUs.removed
decontaminated$mean.per.group
decontaminated$sum.per.group
```

### Reformat decon.table.
 - Convert column 1 to row names.
 - Remove blank average column (1).
 - Save rownames as seperate vector to be added back, as row names are removed during apply().
 - Convert numeric values to integers (for downstream analysis).
 - Transpose data.
 
```{r, warning=F, message=F,eval=F}
seqtab.microdecon <- decontaminated$decon.table %>% 
  remove_rownames() %>% 
  column_to_rownames(var = "ID") %>% 
  select(-1) %>% # remove mean blank
  as.matrix() %>% 
  t()
```

## Assign taxonomy.
 - With optional species addition (there is an agglomeration step downstream, so you can add species now for curiosities sake, and remove later for analysis).
 
```{r, warning=F, message=F,eval=F}
taxa <- assignTaxonomy(seqtab.microdecon,
        "C:/Users/Jacob/Desktop/Jacob_Uni/Data/SCN_vs_NICU_probiotic_study/Data/silva_nr_v132_train_set.fa.gz")

taxa.print <- taxa # Removes sequence rownames for display only
rownames(taxa.print) <- NULL

apply(taxa, 2, function(col)sum(is.na(col))/length(col)) * 100
```

### Compare taxonomic assignment options

```{r, warning=F, message=F,eval=F}
taxa2 <- assignTaxonomy(seqtab.microdecon,
        "C:/Users/Jacob/Desktop/Jacob_Uni/Data/SCN_vs_NICU_probiotic_study/Data/silva_nr99_v138_train_set.fa.gz")

apply(taxa2, 2, function(col)sum(is.na(col))/length(col)) * 100
```


```{r, warning=F, message=F,eval=F}
taxa3 <- assignTaxonomy(seqtab.microdecon, "data/silva_nr99_v138.1_train_set.fa.gz")

apply(taxa3, 2, function(col)sum(is.na(col))/length(col)) * 100
```
**DELETE THE VERSION THAT HAS LESS TAXA?**





# Preprocessing: Creating a Phyloseq Object.

## About.
Creating a [phyloseq](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0061217) object to be used for analysis, and create different objects to be used for different types of analysis downstream.

## Load required packages. 

```{r, warning=F, message=F, results='hide'}
sapply(c("caret", "pls", "e1071", "ggplot2", 
    "randomForest", "tidyverse", "ggrepel", "nlme", "devtools", 
    "reshape2", "PMA", "structSSI", "ade4","ggnetwork", 
    "intergraph", "scales", "readxl", "genefilter", "impute", 
    "phyloseq", "phangorn", "dada2", "DECIPHER", "gridExtra", "stringi", "janitor"), 
    require, character.only = TRUE)
```

## Import metadata.

```{r, warning=F, message=F, eval=F}
Metadata <- readxl::read_excel("data/metadata/Metadata.xlsx") %>% 
  filter(ID != "NO SAMPLE", Study != "BLANK") %>% 
  mutate(row_names = ID) %>% 
  column_to_rownames("row_names") %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(Sample_Type = ifelse(OMNIgene_Number == "Original", "Original", "OMNIgene")) 
```





## De-identify **REMOVE FROM UPLOADED SCRIPT**
```{r, warning=F, message=F, eval=F}
Metadata <- Metadata %>% 
  mutate(URN = as.character(URN)) %>%
  mutate(URN = ifelse(URN == "647780", "1", URN),
         URN = ifelse(URN == "648195", "2", URN),
         URN = ifelse(URN == "648890", "3", URN),
         URN = ifelse(URN == "957821", "4", URN),
         URN = ifelse(URN == "957844", "5", URN)) %>% 
  mutate(URN = as.factor(URN))
```





## Constrcut the Phyloseq object.
 - Includes: metadata, ASV table, taxonomy table and phylogenetic tree.
 
```{r, warning=F, message=F, results='hide', eval=F}
ps <- phyloseq(otu_table(seqtab.microdecon, taxa_are_rows=FALSE), 
               sample_data(Metadata), 
               tax_table(taxa2))
```


## Getting read counts
```{r, eval=F}
sample_data(ps) %>% 
  unclass() %>% 
  as.data.frame() %>% 
  mutate(TotalReads = sample_sums(ps)) %>% 
  ggplot(aes(TotalReads)) + 
    geom_histogram() + 
    ggtitle("Sequencing Depth")
```

## Filtering and normalisation.

### Taxonomy filtering.
 - Can check the number of phyla before and after transformation with `table(tax_table(ps)[, "Phylum"], exclude = NULL)`.
 - Remove features with ambiguous and NA phylum annotation.

```{r, warning=F, message=F, results='hide', eval=F}
ps1 <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
```

#### Check percentages of NA values left.
```{r,warning=F,message=F}
# Total
sum(is.na(tax_table(ps1)))/prod(dim(tax_table(ps1))) * 100
```

```{r,warning=F,message=F}
# Per taxonomic rank
apply(tax_table(ps1), 2, function(col)sum(is.na(col))/length(col)) * 100
```

### Filtering.
 - Using an unsupervised method (relying on the data in this experiment) explore the prevelance of features in the dataset.
 - Calculate the prevalence of each feature and store as a dataframe.
 - Add taxonomy and total read counts.
 - Compare prevelance and abundance filtering before deciding on the optimal method.
 
```{r, warning=F, message=F, results='hide', eval=F}
prevdf = apply(X = otu_table(ps1),
               MARGIN = ifelse(taxa_are_rows(ps1), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})

prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(ps1),
                    tax_table(ps1))
```

 - Plot the relationship between prevelance and total read count for each feature. This provides information on outliers and ranges of features.
 
```{r, warning=F, message=F, fig.cap="Scatterplot exploring the relationship between prevelance and abundance of phyla."}
prevdf %>%
  subset(Phylum %in% get_taxa_unique(ps1, "Phylum")) %>%
  ggplot(aes(TotalAbundance, Prevalence / nsamples(ps1),color=Phylum)) +
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 1) +  
  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) + theme(legend.position="none")
```

 - Define prevalence threshold based on the plot and apply to ps object (if prevelance is too low don't designate a threshold).
 
```{r, warning=F, message=F, results='hide', eval=F}
prevalenceThreshold = 0.01 * nsamples(ps1)

keepTaxa = rownames(prevdf)[(prevdf$Prevalence >= prevalenceThreshold)]

ps2 = prune_taxa(keepTaxa, ps1)

as.data.frame(keepTaxa) %>%  nrow()
```
 
 - Define abundance threshold
 
```{r, warning=F, message=F, results='hide', eval=F}
prevalenceThreshold = prevdf %>% 
  summarise(sum(TotalAbundance)*.00001) %>% 
  pull(1)

keepTaxa = prevdf %>% 
  filter(TotalAbundance >= prevalenceThreshold) %>% 
  rownames()

ps2 = prune_taxa(keepTaxa, ps1)

as.data.frame(keepTaxa) %>%  nrow()
```

**Used abundance threshold.**

 - Explore the relationship on the filtered data set.
```{r, warning=F, message=F, fig.cap="Scatterplot exploring the relationship between prevelance and abundance of phyla on data passed through a prevalence threshold."}
prevdf %>%
  subset(Phylum %in% get_taxa_unique(ps2, "Phylum")) %>%
  ggplot(aes(TotalAbundance, Prevalence / nsamples(ps2),color=Phylum)) +
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 1) +  
  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) + theme(legend.position="none")
```

#### Check percentages of NA values left.
```{r,warning=F,message=F}
# Per taxonomic rank
apply(tax_table(ps2), 2, function(col)sum(is.na(col))/length(col)) * 100
```

### Aggolmerate taxa.
 - Combine features that descend from the same genus as most species have not been identified due to the poor taxonomic depth in 16S, a result of the length of the fragment amplified from the 16SrRNA gene.  
 - Can check how many genera would be present after filtering by running `length(get_taxa_unique(ps2, taxonomic.rank = "Genus"))` and/or `ntaxa(ps3)` will give the number of post agglomeration taxa.
 
```{r, warning=F, message=F, results='hide', eval=F}
ps3 = tax_glom(ps2, "Genus", NArm = FALSE) 
```

#### Check percentages of NA values left.
```{r,warning=F,message=F}
# Per taxonomic rank
apply(tax_table(ps3), 2, function(col)sum(is.na(col))/length(col)) * 100 
```

### Normalisation.
 - Plot a refraction curve to see if total sum scaling will surfice.
 - Define colours and lines.
 - Step = step size for sample sizes in rarefaction curve.
 
```{r, warning=F, message=F, eval=F}
vegan::rarecurve(t(otu_table(ps3)), step = 20, label = FALSE, main = "Rarefaction Curve", 
        col = c("black", "darkred", "forestgreen", "orange", "blue", "yellow", "hotpink"))
```

 - Perform total sum scaling on agglomerated dataset.
 
```{r, warning=F, message=F, eval=F}
ps4 <- transform_sample_counts(ps3, function(x) x / sum(x))
```

 - Explore normalisation with violin plots.
 - Compares differences in scale and distribution of the abundance values before and after transformation.
 - Using arbitrary subset, based on Phylum = Firmicutes, for plotting (ie. can explore any taxa to observe transformation).

```{r, warning=F, message=F, fig.cap="Violin plots exploring of distribution of abundance in Firmicutes before and after normalisation of data."}
plot_abundance = function(physeq, Title = "Abundance", 
                          Facet = "Order", Color = "Phylum", variable = "Study"){
  
    subset_taxa(physeq, Phylum %in% c("Firmicutes")) %>%
    psmelt() %>%
    subset(Abundance > 0) %>%
    ggplot(mapping = aes_string(x = variable, y = "Abundance", color = Color, fill = Color)) +
      geom_violin(fill = NA) +
      geom_point(size = 1, alpha = 0.3, position = position_jitter(width = 0.3)) +
      facet_wrap(facets = Facet) + 
      scale_y_log10()+
      theme(legend.position="none") +
      labs(title = Title)
}

grid.arrange(nrow = 2, (plot_abundance(ps3, Title = "Abundance", 
                          Color = "Study", variable = "Study")),
                        plot_abundance(ps4, Title = "Relative Abundance", 
                          Color = "Study", variable = "Study"))
```
